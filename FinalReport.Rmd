---
title: "Predicting Outcome of DOTA2 Games"
subtitle: "STAT 432 Final Report"
author:
   Will Jeziorski
   Zhiyuan Xie
   and Julian Nieto
date: "May 1, 2019"
output:
  pdf_document: default
  html_document: default
---

```{r set-options, include = FALSE}
# Sets default chunk options

knitr::opts_chunk$set(

  # Figures/Images will be centered

  fig.align = "center", 

  # Code will not be displayed unless `echo = TRUE` is set for a chunk

  fig.width = 3,
  
  fig.height = 3,
  
  # Setting Dimensions 
  
  echo = FALSE,

  # Messages are suppressed

  message = FALSE,

  # Warnings are suppressed

  warning = FALSE

)
```

\centering

\raggedright

\tableofcontents

\clearpage

# Introduction

## Game description

Dota 2 has been one of the most famous multiplayer online battle arena game since 2013. The game has five people play against five other people on two different teams called Radiant and Dire. Player pick heroes, and each hero has unique abilities. The team that wins does so by destroying the enemy base, called the ancient.

## Our goal

For our project, our goal is to find which factors will be important in determining the winner. To do this, we needed to create a data set that allowed us to perform an analysis on which team won for a single game.

## Data Description
 
Our dataset comes from OpenDota.com, and in our project, we specifically use their YASP dataset for doing the analysis. The aggregate dataset contains approximately five- hundred games in 2015, and each game have data of variables is shown below:

* Side of Winner 
* Tower status 
* Number of Chats
* Structure Status
* Creeps Scores  

Link to the data source: magnet:?xt=urn:btih:5c5deeb6cfe1c944044367d2e7465fd8bd2f4acf&tr=http%3A%2F%2Facademictorrents.com%2Fannounce.php&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce

## Literature Review

A team coming from University of California, San Diego has done the a similar analysis based on logistic regression and random forest classifier. They first used post-game statistics, which included gold per minute, XP per minute, and kill per minute. These data are considerably important because the amount of gold can determine which items the player could buy, and XP could show hero’s level status. Using this dataset, their logistic regression and random forest gave them a win rate around 99%. On the other hand, they use only heroes ID from from each side and synergy for each two heroes from each side to construct models, which give them a lower rate about 70%. 


# Summary Statistics and Data Visualization

## Data Cleaning

For this project, we needed to access thousands of games worth of DOTA 2 data. To do this, we first downloaded about Open DOTA website. This website provides information about all aspects of the game, ranging from the macro-game values such as who won the game and what objectives were taken to the micro-game data involving individual creep score and lane position. As a result, one observation contains a lot of data. The first observation of one of our initial data sets was a 14 minute and 58 second game. When we extracted the observation into a list in R we found that it was 145 pages of text in Microsoft Word if we allowed 3 columns per page of observations.

Clearly, we cannot do analysis on a data set this large. As a result, we decided to extract only the variables we thought would influence a win for either side. When we were cleaning the data, we ran into a couple of issues. First, we ran into an issue where there could be games that don’t initialize and don’t end with a winner. Luckily, this caused our function to fail to run, and we were able to drop the putliner. Clearly, there are more variables that we could extract, but these are the ones we decided to analyze given our time constraints. As a result of this cleaning, we shrunk our about 11 gigabytes of data to two files totaling 27 megabytes of data. 

```{r data_init, cache = TRUE}

train_data = read.csv("train.csv")
test_data = read.csv("test.csv")
train_data = train_data[, 2:178]
test_data = test_data[, 2:178]
```

Here is a quick sample of our data. Our training data set and testing data set have the same format. For our training data set, we have 50000 observations, while our testing data set has 10000 observations so we have an 83%/17% split of our data. The data output below is the first 6 observations of the training data set and only the first 15 columns are output from this sample.

```{r, echo = TRUE, eval = TRUE}
head(train_data,6)[,1:15]
```

To get a sense of what are data involves, we created a few charts to familiarize the reader with some of the things our data set holds.

```{r acc_creation, cache = TRUE}
frequency = rep(0, 12)
accuracy = rep(0, 12)

M = rep(0,12)
    
for (i in 1:12){
  M[i] = paste0(i - 1)
}

for (j in 1:50000) {
  i = train_data[j, ]
  frequency[i$RadiantTowerStatus + 1] = frequency[i$RadiantTowerStatus + 1] + 1
  if (i$RadiantWin == 1) {
     accuracy[i$RadiantTowerStatus + 1] = accuracy[i$RadiantTowerStatus + 1] + 1
  }
}
accuracy = accuracy  / frequency
```

```{r tower_barplot, echo = TRUE}
barplot(accuracy, names.arg = M, xlab="Number of Radiant Towers",ylab="Win Rate",col="blue",
main="Win Rate VS Number of Towers")


```

```{r accfreqcreate, cache = TRUE}
frequency = rep(0, 110)
accuracy = rep(0, 110)

M = rep(0,110)
    
for (i in 1:110){
  M[i] = paste0(i)
}

names(accuracy) = M
for (j in 1:50000) {
  i = train_data[j, ]
  heros = train_data[j, seq(8, 117)]
  idxes = which(heros != 0, arr.ind = TRUE)
  frequency[idxes[, 2]] = frequency[idxes[, 2]] + 1
  if (i$RadiantWin == 1) {
    idxesWin = which( heros == 1, arr.ind=TRUE)
    accuracy[idxesWin[, 2]] = accuracy[idxesWin[, 2]] + 1
  }
  else {
    idxesWin = which( heros == -1, arr.ind=TRUE)
    accuracy[idxesWin[, 2]] = accuracy[idxesWin[, 2]] + 1
  }
}

accuracy = accuracy  / frequency

```

```{r, winrateplot, echo = TRUE}
barplot(sort(accuracy, decreasing = TRUE), ylim = c(0, .60), xlab="Hero ID",ylab="Win Rate",col="blue",
main="Win Rate VS Hero")

#axis(2, seq(0, .65, .05), c(0, .05,.1,.15,.2,.25,.3,.35,.4,.45,.5,.55,.6, .65))
```

```{r pickrate, cache = TRUE}
#par(mar = c(6.5, 6.5, 0.5, 0.5), mgp = c(5, 1, 0))
acc = frequency / 50000
names(acc) = M

```

```{r, echo = TRUE}

barplot(sort(acc, decreasing = TRUE), ylim = c(0, .35), xlab="Hero ID",ylab="pick Rate",col="blue",
main="Hero pick rate")

#axis(2, seq(0, .5, .05), c(0, .05,.1,.15,.2,.25,.3,.35,.4,.45,.5))

```

```{r pickwin, cache = TRUE}

pickVsWin = cbind(acc, accuracy)

data = pickVsWin[order(acc, decreasing = TRUE), ]

```

```{r winratevspick, echo = TRUE}
plot(data[, 1], data[, 2], xlab="Pick rate",ylab="Win Rate",col="blue",
main="Hero pick rate")
```

Summary Here

# Proposed Analysis

## Neural Network

We passed our data through the a neural network, however we quickly ran into runtime issues. Therefore, we could only run a 3 layer network at the most as a 4-layer network would never complete or converge. We also could have a large hidden network as anything really more than 3 would take hours to run. We had to leave it at 3 hidden nodes because of that  meaning the accuracy and internal representation were severely harmed. In the end using all the data as input we obtained a  61.65% accuracy. I believe if we had a more powerful computer and more time to run, we could add more layers and nodes to get a higher accuracy.

## Logistic Regression

Since, the neural network failed to accurately predict the winner. We moved onto logistic regression because logistic regression predicts a probability, so if the probability was greater a half we predicted a radiant win else a dire win. When we used all the variables we acquired an accuracy  of 98.47% on the testing data, which is obviously really good. When we looked at the significances of the variables we found that the hero picks were mostly not significant and did not explain a lot. We currently have all the heroes as an indicator variable making the data really sparse maybe there could be a better way of representing it. With this information, we ran the logistic regression again but without the hero id data and with that we got 98.34% accuracy. So losing the hero data did not impact the accuracy a lot. Some of the variables that were the most important predictors were the number of towers, number of barracks, deaths, team that got first blood, and kills by each team. Something interesting was the deaths were actually more significant than kills on first glance, we thought that they would be similar in significance. I believe that deaths are more important because they explain the deaths by external forces that are not enemy players such as creeps, towers, and jungle creeps. This is also the same reason why damage taken by a player is more significant than damage inflicted, because of non-player damage sources. The variables creep score and denies were not that significant compared to the others; because the roles (carry, support, and jungle) the players took was not considered in our data so the information carried by creep score and denies was diminished.

# Conclusion and Discussion

## Discussion

Our project is not without issues. One thing our data set does not identify is the role of each player. In DOTA, each player generally plays a different role in the game for their team. However, our original JSON file did not contain the information pertaining to the role the player played, as in DOTA this is not set by the game but by the players themselves. This may be the reason why creep score and denies were insignificant, among other things. Perhaps if we created an accumulation variable per team we would see a more significant difference. Additionally, our “times chatted” variable did not differentiate between who sent the message, but rather just counted the number of messages sent in the game. As a result, it doesn’t entirely capture the effect chatting has on a team’s ability to win. It could also be argued that “times chatted” depends on which side is winning, rather than how we present it in the model. This opens up the door to future analysis on how chatting affects (or is affected by) how a team is performing in game.

Additionally, we wanted our neural net to have more hidden nodes, but we could not support the computational time needed to do so with our data. If we cut down the size of our data set a bit we may have been able to get a neural net that provided a better fit than the one we used.

## Conclusion

From our use of a Logistic Regression, we found that the best indicators for which team won a game were in general deaths, then wins. We found that variables such as times chatted and the creep score and denies were rather insignificant when predicting the outcome. Our logistic regression was incredibly good at classifying who won the match, with a less than 2% error rate for both of our models. The neural net performed a lot worse, having a 38.35% error rate. 




