---
title: "Predicting Outcome of DOTA2 Games"
author: "Will Jeziorski Zhiyuan Xie and Julian Nieto"
date: "May 1, 2019"
output:
  html_document: default
  pdf_document: default
subtitle: STAT 432 Final Report
---

```{r set-options, include = FALSE}
# Sets default chunk options

knitr::opts_chunk$set(

  # Figures/Images will be centered

  fig.align = "center", 

  # Code will not be displayed unless `echo = TRUE` is set for a chunk

  fig.width = 3,
  
  fig.height = 3,
  
  # Setting Dimensions 
  
  echo = FALSE,

  # Messages are suppressed

  message = FALSE,

  # Warnings are suppressed

  warning = FALSE

)
```

\centering

\raggedright

\tableofcontents

\clearpage

# Introduction

Before we dive into the paper, we want to provide the reader with a link to our data cleaning rmd file, as it is rather large and we don't want to include it in our paper for space considerations. We also provide the reader with two more links in the repository which contain both the training and testing data set.

Link for Data Cleaning File: https://github.com/JulianFN/Stat432Project/blob/master/Project%20Data%20Cleaning%20File.Rmd

Link for Training Data: https://github.com/JulianFN/Stat432Project/blob/master/train.csv

Link for Testing Data: https://github.com/JulianFN/Stat432Project/blob/master/test.csv

## Game description

Dota 2 is one of the most famous multiplayer online battle arena games since 2013. The game has five people play against five other people on two different teams called Radiant and Dire.The players pick heroes, with each hero having a set of unique abilities. The team that wins does so by destroying the enemy base, called the ancient.

## Our goal

For our project, our goal is to find which factors will be important in determining the winner. To do this, we needed to create a data set that allowed us to perform an analysis on which team won for a single game. After creating this data set, we want to perform various binary classification techniques such as neural networks and logistic regression. 

## Data Description
 
Our dataset comes from OpenDota.com, and in our project, we specifically use their YASP dataset for doing the analysis. The aggregate dataset contains approximately five-hundred thousand games (Of which we took 60,000) in 2015,each game variables such as.

* Team that Won 
* Tower status (Number of towers left at the end)
* Times Chatted
* Structure Status (Number of barracks left at the end)
* Player Data (Which contained this for each player)
    * Creep Score
    * Deaths
    * Kills
    * Denies
    * Hero 
    * Items

This data was initially contained in a JSON file that was about 62 GB large. We then used the previously mentioned data cleaning file to clean the data and then dropped the first column of the data (as it was just X values.) 

The link to the source of the data set we used is in the references section of this paper.

## Literature Review

Nicholas Kinkade and Kyung yul Kevin Lim from University of California at San Diego has done the a similar analysis based on logistic regression and random forest classifier. They first used post-game statistics, which included gold per minute, XP per minute, and kill per minute. These data are considerably important because the amount of gold can determine which items the player could buy, and XP could show hero’s level status. Using this dataset, their logistic regression and random forest gave them a win rate around 99%. They also used the hero picks from from each side and calculated the synergy between the team's heroes from each side. They even added how much the enemy's countered the other's team by esssentially calculating how effective each hero was against another. Using the only heros picks, synergy, and countering statistics the team was able to obtain a 70% prediction rate, which is impressive considering they did not use any post or during game variables to help only the starting configuration.


# Summary Statistics and Data Visualization

## Data Cleaning

For this project, we needed to access thousands of games worth of DOTA 2 data. To do this, we first downloaded about Open DOTA website. This website provides information about all aspects of the game, ranging from the macro-game values such as who won the game and what objectives were taken to the micro-game data involving individual creep score and lane position. As a result, one observation contains a lot of data. The first observation of one of our initial data sets was a 14 minute and 58 second game. When we extracted the observation into a list in R we found that it was 145 pages of text in Microsoft Word if we allowed 3 columns per page of observations.

Clearly, we cannot do analysis on a data set this large. As a result, we decided to extract only the variables we thought would influence a win for either side. When we were cleaning the data, we ran into a couple of issues. First, we ran into an issue where there could be games that don’t initialize and don’t end with a winner. Luckily, this caused our function to fail to run, and we were able to drop the outliner. Clearly, there are more variables that we could extract, but these are the ones we decided to analyze given our time constraints. As a result of this cleaning, we shrunk our about 11 gigabytes of data to two files totaling 27 megabytes of data. 

```{r data_init, cache = TRUE}

train_data = read.csv("train.csv")
test_data = read.csv("test.csv")
train_data = train_data[, 2:178]
test_data = test_data[, 2:178]
```

Here is a quick sample of our data. Our training data set and testing data set have the same format. For our training data set, we have 50000 observations, while our testing data set has 10000 observations so we have an 83%/17% split of our data. The data output below is the first 6 observations of the training data set and only the first 15 columns are output from this sample.

```{r, echo = TRUE, eval = TRUE}
head(train_data,6)[,1:15]
```

To get a sense of what are data involves, we created a few charts to familiarize the reader with some of the things our data set holds.

First, we created a bar plot visualizing the win rate vs number of towers. This graph will help the reader see how towers affect the winrate.

```{r acc_creation, cache = TRUE}
frequency = rep(0, 12)
accuracy = rep(0, 12)

M = rep(0,12)
    
for (i in 1:12){
  M[i] = paste0(i - 1)
}

for (j in 1:50000) {
  i = train_data[j, ]
  frequency[i$RadiantTowerStatus + 1] = frequency[i$RadiantTowerStatus + 1] + 1
  if (i$RadiantWin == 1) {
     accuracy[i$RadiantTowerStatus + 1] = accuracy[i$RadiantTowerStatus + 1] + 1
  }
}
accuracy = accuracy  / frequency
```

```{r tower_barplot, echo = TRUE}
barplot(accuracy, names.arg = M, xlab="Number of Radiant Towers",ylab="Win Rate",col="blue",
main="Win Rate VS Number of Towers")


```

In general, we see that win the radiant has more towers they end up winning more. This makes perfect sense, as towers prevent the enemy team from running into the base and destroying it.

Next, we created a bar plot showing how win rate differed between heroes. 

```{r accfreqcreate, cache = TRUE}
frequency = rep(0, 110)
accuracy = rep(0, 110)

M = rep(0,110)
    
for (i in 1:110){
  M[i] = paste0(i)
}

names(accuracy) = M
for (j in 1:50000) {
  i = train_data[j, ]
  heros = train_data[j, seq(8, 117)]
  idxes = which(heros != 0, arr.ind = TRUE)
  frequency[idxes[, 2]] = frequency[idxes[, 2]] + 1
  if (i$RadiantWin == 1) {
    idxesWin = which( heros == 1, arr.ind=TRUE)
    accuracy[idxesWin[, 2]] = accuracy[idxesWin[, 2]] + 1
  }
  else {
    idxesWin = which( heros == -1, arr.ind=TRUE)
    accuracy[idxesWin[, 2]] = accuracy[idxesWin[, 2]] + 1
  }
}

accuracy = accuracy  / frequency

```

```{r winrateplot, echo = TRUE, fig.height = 4, fig.width = 6}
barplot(sort(accuracy, decreasing = TRUE), ylim = c(0, .60), xlab="Hero ID",ylab="Win Rate",col="blue",
main="Win Rate VS Hero")
```

From this plot, we identified that Chen, Clinkz, and Skywrath Mage all had the highest winrate with about 58%, while Storm Spirit, Puck, and Night Stalker had the lowest winrates each with about 41%. Having played some DoTa, Storm Spririt and Puck are not terrible heros but they are hard heroes to learn leading them to have low win rates. The differences in win rates arent too drastic with the highest chances being 58% however the difference between the top and bottom heros is 17%, meaning over a lot of games you will notice the difference. However just using the hero win rates as a predictor would still be bascally a coin toss.

We next made a similar plot that detailed which heroes were the most popular.

```{r pickrate, cache = TRUE}
acc = frequency / 50000
names(acc) = M
```

```{r heroplot, echo = TRUE, fig.height = 4, fig.width = 5}

barplot(sort(acc, decreasing = TRUE), ylim = c(0, .35), xlab="Hero ID",ylab="pick Rate",col="blue",
main="Hero pick rate")
```

From this data, we see that Shadow Fiend, Windrunner, and Alchmist are the most picked with 32%, 30%, and 23%. This means that Shadow Fiend was in 32% of every game we looked, which is high considering there are 100+ heros to chose from. The least picked were Jakiro, Lycan, and Abaddon each with about a 2% pick rate so a game with these heros in the line ups only has a 2% chance of happening.
```{r pickwin, cache = TRUE}

pickVsWin = cbind(acc, accuracy)

datat = pickVsWin[order(acc, decreasing = TRUE), ]
datat = as.data.frame(datat)

```

```{r winratevspick, echo = TRUE, fig.width=10, fig.height=4}

par(mfrow = c(1,2))
plot(datat$acc, datat$accuracy, xlab="Pick rate",ylab="Win Rate",col="blue",
main="Hero Win Rate vs Pick Rate")

require(splines)
fit.bs = lm(accuracy ~ bs(acc, df=6), data=datat)
plot(datat$acc, datat$accuracy, pch = 19, xlab = "Pick rate", ylab = "Win rate", col = "darkorange")   
lines(seq(0.02, .32,.01), predict(fit.bs, data.frame("acc"= seq(0.02, .32,.01))), col="deepskyblue", lty=1, lwd = 3)    
```

For our next plot we hoped to capture the relationship between being picked and winning. The relationship is not that strong between the two but there is a slight trend. The lower picked heros do seem to have a slightly below average winrate. It seems to curve up and peak around a 10% pickrate with all the highest winrate heros lying there. Then it tappers of to around a 50% winrate the higher the pickrate. Overall, I would say the relationship between winrate and pick rate is weak

# Proposed Analysis

For our analysis, we decided to focus on assigning a probability to "RadiantWin" taking a value of 1. Then, we would use this probability to assign values of 1 or 0 to the supplied testing data set, and then we would create a confusion matrix to see how well our models worked. For this paper, we focused on two different methods of this type of binary classification: Neural Network and Logistic Regression. Both of these methods will apply probabilities to our outcome variable, which we can then use to classify our results into each category.

## Neural Network

To begin our analysis, we first wanted to try a nerual network. We passed our data through the a neural network, however we quickly ran into runtime issues. Therefore, we could only run a 3 layer network at the most as a 4-layer network would never complete or converge. We also could not have a large hidden network as anything really more than 3 nodes would take hours to run. We had to leave it at 3 hidden nodes because of that  meaning the accuracy and internal representation were severely harmed. In the end using all the data as input we obtained a  66.01% accuracy. We believe if we had a more powerful computer and more time to run, we could add more layers and nodes to get a higher accuracy.

```{r neuralnet, cache= TRUE, echo = TRUE}
library(neuralnet)
set.seed(42)
new_train_data = train_data[, -seq(8, 117)]
new_test_data = test_data[, -seq(8, 117)]
nn=neuralnet(`RadiantWin`~ ., data = train_data, hidden= 3,act.fct = "logistic",
                linear.output = FALSE, stepmax = 10000)

nn_conf_mat = table(test_data$RadiantWin, as.numeric(predict(nn,test_data)>=.5))

nn_conf_mat

(nn_conf_mat[1,1] + nn_conf_mat[2,2])/nrow(test_data)
```


## Logistic Regression

Since, the neural network failed to accurately predict the winner. We moved onto logistic regression because logistic regression predicts a probability, so if the probability was greater a half we predicted a radiant win else a dire win. When we used all the variables, we acquired an accuracy  of 98.47% on the testing data, which is obviously really good. When we looked at the significances of the variables we found that the hero picks were mostly not significant and did not explain a lot. We think that is because, we currently have all the heroes as an indicator variable making the data really sparse maybe there could be a better way of representing it. With this information, we ran the logistic regression again but without the hero data and with that we got 98.34% accuracy. Thus, removing the hero data did not impact the accuracy a lot. Some of the variables that were the most important predictors were the number of towers, number of barracks, deaths, team that got first blood, and kills by each team. Something interesting about that was that the deaths were actually more significant than kills, on first glance we thought that they would be similar in significance. However, revisiting it made us come to the conclusion that deaths are more important because they explain the deaths by external forces that are not enemy players such as creeps, towers, and jungle creeps. This is also the same reason why damage taken by a player is more significant than damage inflicted, because of non-player damage sources. The variables creep score and denies were not that significant compared to the others, because the roles (carry, support, and jungle) the players took were not considered in our data so the information carried by creep score and denies was diminished.

```{r logitheroes, echo = TRUE, cache = TRUE}

mylogit <- glm(`RadiantWin` ~ ., data = train_data, family = "binomial")

summary(mylogit)$coefficients[c(3:6,118:157),]

log_conf_mat=table(test_data$RadiantWin, as.numeric(predict(mylogit,test_data)>=.5))

log_conf_mat

(log_conf_mat[1,1] + log_conf_mat[2,2])/nrow(test_data)
```

```{r logitnohero, echo = TRUE, cache = TRUE}
mylogitnh <- glm(`RadiantWin` ~ ., data = new_train_data, family = "binomial")

log_conf_mat_nh=table(new_test_data$RadiantWin, as.numeric(predict(mylogitnh,new_test_data)>=.5))

log_conf_mat_nh

(log_conf_mat_nh[1,1] + log_conf_mat_nh[2,2])/nrow(new_test_data)

```

Regression output for the data without heroes is omitted due to being approximately the same, since the hero data didn't add much to our prediction ability.

# Conclusion and Discussion

## Conclusion

From our use of a Logistic Regression, we found that the best indicators for which team won a game were deaths, wins, and the structure status status for each team. We found that variables such as times chatted and the creep score and denies were rather insignificant when predicting the outcome. Our logistic regression was incredibly good at classifying who won the match, with a less than 2% error rate for both of our models. The neural net performed a lot worse, having a 38.35% error rate. 


## Discussion

Our project is not without issues. One thing our data set does not identify is the role of each player. In DOTA, each player generally plays a different role in the game for their team. However, our original JSON file did not contain the information pertaining to the role the player played, as in DOTA this is not set by the game but by the players themselves. This may be the reason why creep score and denies were insignificant, among other things. Perhaps if we created an accumulation variable per team we would see a more significant difference. Additionally, our “times chatted” variable did not differentiate between who sent the message, but rather just counted the number of messages sent in the game. As a result, it doesn’t entirely capture the effect chatting has on a team’s ability to win. It could also be argued that “times chatted” depends on which side is winning, rather than how we present it in the model. This opens up the door to future analysis on how chatting affects (or is affected by) how a team is performing in game.

Additionally, we wanted our neural net to have more hidden nodes, but we could not support the computational time needed to do so with our data. If we cut down the size of our data set a bit we may have been able to get a neural net that provided a better fit than the one we used.

If we had the computational ability and time to do so, we would also be interested in looking at if certain combinations of heroes were significant to determining whether or not a team won. There are instances where certain heroes synergize well when played together. There is also the case where certain heroes counter each other in game, so that could change the winrate as well. Thus, it would be extremely interesting to see if these hero combinations changed our analysis at all, especially the logistic regression with hero-level data.

\clearpage

# References

Kinkade, Nicholas and Kyung yul Kevin Lim (n.d): "DOTA 2 Win Prediction" _UC San Diego_ 1-7.

"OpenDota Data Dump", OpenDota Blog. Retrieved from https://blog.opendota.com/2015/12/20/datadump/